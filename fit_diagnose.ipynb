{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 4573338 characters\n",
      "Vocabulary size: 67 characters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras import optimizers\n",
    "import argparse\n",
    "from RNN_utils import *\n",
    "\n",
    "DATA_DIR = './data/shakespeare_input.txt'\n",
    "BATCH_SIZE = 30\n",
    "HIDDEN_DIM = 300\n",
    "SEQ_LENGTH = 50\n",
    "\n",
    "lossfile = './data/warpeace_loss.txt'\n",
    "\n",
    "GENERATE_LENGTH = 100\n",
    "LAYER_NUM = 2\n",
    "\n",
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_char, char_to_ix = load_data(DATA_DIR, SEQ_LENGTH)\n",
    "\n",
    "# Creating and compiling the Network\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "  model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "rms_prop = optimizers.RMSprop(lr=1e-4)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=rms_prop)\n",
    "\n",
    "# Generate some sample before training to know how bad it is!\n",
    "# print(\"\\n\\nGenerating untrained text...\\n\\n\")\n",
    "# generate_text(model, GENERATE_LENGTH, VOCAB_SIZE, ix_to_char, -1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=2,\n",
    "                              verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73172 samples, validate on 18294 samples\n",
      "Epoch 1/1\n",
      "73172/73172 [==============================] - 410s 6ms/step - loss: 2.9432 - val_loss: 2.5200\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=BATCH_SIZE, validation_split=0.2, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
