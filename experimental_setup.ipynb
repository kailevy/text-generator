{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import keras.callbacks\n",
    "from keras import optimizers\n",
    "import argparse\n",
    "import pickle\n",
    "from RNN_utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 4573338 characters\n",
      "Vocabulary size: 67 characters\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_LAYERS = 2\n",
    "DEFAULT_HIDDEN = 500\n",
    "DEFAULT_DROPOUT = 0.0\n",
    "\n",
    "layers = [1, 3]\n",
    "hidden = [300, 700]\n",
    "dropout = [0.15, 0.3]\n",
    "DATA_DIR = './data/shakespeare_input.txt'\n",
    "BATCH_SIZE = 30\n",
    "SEQ_LENGTH = 50\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_char, char_to_ix = load_data(DATA_DIR, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split testing data from training/validation data\n",
    "train_split = 0.8\n",
    "test_ind = int(round(train_split*len(X)))\n",
    "X_test = X[test_ind:]\n",
    "y_test = y[test_ind:]\n",
    "X_train = X[:test_ind]\n",
    "y_train = y[:test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_model(VOCAB_SIZE, num_layers, num_hidden, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(num_hidden, input_shape=(None, VOCAB_SIZE), return_sequences=True, dropout=dropout))\n",
    "    for i in range(num_layers - 1):\n",
    "      model.add(LSTM(num_hidden, return_sequences=True,dropout=dropout))\n",
    "    model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, X, y, num_epochs=50, batch_size=30):\n",
    "    early_stop1 = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=2,\n",
    "                              verbose=1, mode='auto')\n",
    "    early_stop2 = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                              min_delta=0,\n",
    "                              patience=2,\n",
    "                              verbose=1, mode='auto')\n",
    "    history = model.fit(X, y, batch_size=batch_size, callbacks=[early_stop1, early_stop2], validation_split=0.2, epochs=num_epochs, verbose=1)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, history, num_layers, num_hidden, dropout):\n",
    "    string = 'layers_{}_hidden_{}_dropout_{}_epoch_{}'.format(num_layers, num_hidden, dropout, len(history['loss']))\n",
    "    with open('history'+string, 'wb') as file_pi:\n",
    "        pickle.dump(history, file_pi)\n",
    "    model.save_weights('weights'+string+'.hdf5'.format(num_layers, num_hidden, dropout, len(history['loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model = make_lstm_model(VOCAB_SIZE, DEFAULT_LAYERS, DEFAULT_HIDDEN, DEFAULT_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'default_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-69c61b52aa7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'default_model' is not defined"
     ]
    }
   ],
   "source": [
    "default_model, default_history = run_model(default_model, X_train, y_train)\n",
    "save_model(default_model, default_history, DEFAULT_LAYERS, DEFAULT_HIDDEN, DEFAULT_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    model = make_lstm_model(VOCAB_SIZE, layer, DEFAULT_HIDDEN, DEFAULT_DROPOUT)\n",
    "    model, history = run_model(model, X_train, y_train)\n",
    "    history['test_loss_predict'] = evaluate_loss(model, X_test)\n",
    "    history['test_loss_eval'] = model.evaluate(X_test, y_test, batch_size=30)\n",
    "    save_model(model, history, layer, DEFAULT_HIDDEN, DEFAULT_DROPOUT)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in hidden:\n",
    "    model = make_lstm_model(VOCAB_SIZE, DEFAULT_LAYERS, h, DEFAULT_DROPOUT)\n",
    "    model, history = run_model(model, X_train, y_train)\n",
    "    history['test_loss_predict'] = evaluate_loss(model, X_test)\n",
    "    history['test_loss_eval'] = model.evaluate(X_test, y_test, batch_size=30)\n",
    "    save_model(model, history, DEFAULT_LAYERS, h, DEFAULT_DROPOUT)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dropout:\n",
    "    model = make_lstm_model(VOCAB_SIZE, DEFAULT_LAYERS, DEFAULT_HIDDEN, d)\n",
    "    model, history = run_model(model, X_train, y_train)\n",
    "    history['test_loss_predict'] = evaluate_loss(model, X_test)\n",
    "    history['test_loss_eval'] = model.evaluate(X_test, y_test, batch_size=30)\n",
    "    save_model(model, history, DEFAULT_LAYERS, DEFAULT_HIDDEN, d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
