{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import keras.callbacks\n",
    "from keras import optimizers\n",
    "import argparse\n",
    "import pickle\n",
    "from RNN_utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 4573338 characters\n",
      "Vocabulary size: 67 characters\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_LAYERS = 2\n",
    "DEFAULT_HIDDEN = 500\n",
    "DEFAULT_DROPOUT = 0.0\n",
    "\n",
    "layers = [1, 3]\n",
    "hidden = [300, 700]\n",
    "dropout = [0.15, 0.3]\n",
    "DATA_DIR = './data/shakespeare_input.txt'\n",
    "BATCH_SIZE = 30\n",
    "SEQ_LENGTH = 50\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# Creating training data\n",
    "X, y, VOCAB_SIZE, ix_to_char, char_to_ix = load_data(DATA_DIR, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split testing data from training/validation data\n",
    "train_split = 0.8\n",
    "test_ind = int(round(train_split*len(X)))\n",
    "X_test = X[test_ind:]\n",
    "y_test = y[test_ind:]\n",
    "X_train = X[:test_ind]\n",
    "y_train = y[:test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_model(VOCAB_SIZE, num_layers, num_hidden, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(num_hidden, input_shape=(None, VOCAB_SIZE), return_sequences=True, dropout=dropout))\n",
    "    for i in range(num_layers - 1):\n",
    "      model.add(LSTM(num_hidden, return_sequences=True,dropout=dropout))\n",
    "    model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, X, y, num_epochs=50, batch_size=30):\n",
    "    early_stop1 = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=1e-3,\n",
    "                              patience=2,\n",
    "                              verbose=1, mode='auto')\n",
    "    early_stop2 = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                              min_delta=1e-3,\n",
    "                              patience=2,\n",
    "                              verbose=1, mode='auto')\n",
    "    history = model.fit(X, y, batch_size=batch_size, callbacks=[early_stop1, early_stop2], validation_split=0.2, epochs=num_epochs, verbose=1)\n",
    "    return model, history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, history, num_layers, num_hidden, dropout):\n",
    "    string = 'layers_{}_hidden_{}_dropout_{}_epoch_{}'.format(num_layers, num_hidden, int(dropout*10), len(history['loss']))\n",
    "    print('saving: ' + string)\n",
    "    with open('history_'+string, 'wb') as file_pi:\n",
    "        pickle.dump(history, file_pi)\n",
    "    model.save_weights('weights_'+string+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model = make_lstm_model(VOCAB_SIZE, DEFAULT_LAYERS, DEFAULT_HIDDEN, DEFAULT_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refore,\n",
      "'Riest Soly Bricher and, a land shall,\n",
      "On every piece of the off the poison.\n",
      "\n",
      "GLOUCESTER:\n",
      "Lo"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"refore,\\n'Riest Soly Bricher and, a land shall,\\nOn every piece of the off the poison.\\n\\nGLOUCESTER:\\nLoo\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(default_model, 100, VOCAB_SIZE, ix_to_char, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58538 samples, validate on 14635 samples\n",
      "Epoch 1/50\n",
      "58538/58538 [==============================] - 746s 13ms/step - loss: 1.9000 - val_loss: 1.7130\n",
      "Epoch 2/50\n",
      "58538/58538 [==============================] - 746s 13ms/step - loss: 1.4438 - val_loss: 1.6091\n",
      "Epoch 3/50\n",
      "58538/58538 [==============================] - 741s 13ms/step - loss: 1.3576 - val_loss: 1.5810\n",
      "Epoch 4/50\n",
      "58538/58538 [==============================] - 750s 13ms/step - loss: 1.3096 - val_loss: 1.5694\n",
      "Epoch 5/50\n",
      "58538/58538 [==============================] - 748s 13ms/step - loss: 1.2742 - val_loss: 1.5685\n",
      "Epoch 6/50\n",
      "58538/58538 [==============================] - 745s 13ms/step - loss: 1.2442 - val_loss: 1.5691\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "default_model, default_history = run_model(default_model, X_train, y_train)\n",
    "save_model(default_model, default_history, DEFAULT_LAYERS, DEFAULT_HIDDEN, DEFAULT_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58538 samples, validate on 14635 samples\n",
      "Epoch 1/50\n",
      "58538/58538 [==============================] - 305s 5ms/step - loss: 2.0653 - val_loss: 1.8772\n",
      "Epoch 2/50\n",
      "58538/58538 [==============================] - 304s 5ms/step - loss: 1.5839 - val_loss: 1.7046\n",
      "Epoch 3/50\n",
      "58538/58538 [==============================] - 305s 5ms/step - loss: 1.4603 - val_loss: 1.6520\n",
      "Epoch 4/50\n",
      "58538/58538 [==============================] - 304s 5ms/step - loss: 1.3983 - val_loss: 1.6256\n",
      "Epoch 5/50\n",
      "58538/58538 [==============================] - 302s 5ms/step - loss: 1.3586 - val_loss: 1.6112\n",
      "Epoch 6/50\n",
      "58538/58538 [==============================] - 302s 5ms/step - loss: 1.3293 - val_loss: 1.6039\n",
      "Epoch 7/50\n",
      "58538/58538 [==============================] - 302s 5ms/step - loss: 1.3059 - val_loss: 1.5968\n",
      "Epoch 8/50\n",
      "58538/58538 [==============================] - 302s 5ms/step - loss: 1.2865 - val_loss: 1.5993\n",
      "Epoch 9/50\n",
      "58538/58538 [==============================] - 302s 5ms/step - loss: 1.2694 - val_loss: 1.6027\n",
      "Epoch 00009: early stopping\n",
      "18293/18293 [==============================] - 24s 1ms/step\n",
      "Train on 58538 samples, validate on 14635 samples\n",
      "Epoch 1/50\n",
      "58538/58538 [==============================] - 1177s 20ms/step - loss: 1.9102 - val_loss: 1.6765\n",
      "Epoch 2/50\n",
      "58538/58538 [==============================] - 1187s 20ms/step - loss: 1.4202 - val_loss: 1.5797\n",
      "Epoch 3/50\n",
      "58538/58538 [==============================] - 1188s 20ms/step - loss: 1.3406 - val_loss: 1.5575\n",
      "Epoch 4/50\n",
      "58538/58538 [==============================] - 1183s 20ms/step - loss: 1.2962 - val_loss: 1.5492\n",
      "Epoch 5/50\n",
      "58538/58538 [==============================] - 1187s 20ms/step - loss: 1.2637 - val_loss: 1.5458\n",
      "Epoch 6/50\n",
      "58538/58538 [==============================] - 1187s 20ms/step - loss: 1.2362 - val_loss: 1.5486\n",
      "Epoch 7/50\n",
      "58538/58538 [==============================] - 1184s 20ms/step - loss: 1.2117 - val_loss: 1.5638\n",
      "Epoch 00007: early stopping\n",
      "18293/18293 [==============================] - 87s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "for layer in layers:\n",
    "    model = make_lstm_model(VOCAB_SIZE, layer, DEFAULT_HIDDEN, DEFAULT_DROPOUT)\n",
    "    model, history = run_model(model, X_train, y_train)\n",
    "    history['test_loss_predict'] = evaluate_loss(model, X_test)\n",
    "    history['test_loss_eval'] = model.evaluate(X_test, y_test, batch_size=30)\n",
    "    save_model(model, history, layer, DEFAULT_HIDDEN, DEFAULT_DROPOUT)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58538 samples, validate on 14635 samples\n",
      "Epoch 1/50\n",
      "58538/58538 [==============================] - 347s 6ms/step - loss: 2.0082 - val_loss: 1.7885\n",
      "Epoch 2/50\n",
      "58538/58538 [==============================] - 332s 6ms/step - loss: 1.5321 - val_loss: 1.6606\n",
      "Epoch 3/50\n",
      "58538/58538 [==============================] - 332s 6ms/step - loss: 1.4277 - val_loss: 1.6209\n",
      "Epoch 4/50\n",
      "58538/58538 [==============================] - 330s 6ms/step - loss: 1.3763 - val_loss: 1.6035\n",
      "Epoch 5/50\n",
      "58538/58538 [==============================] - 330s 6ms/step - loss: 1.3424 - val_loss: 1.5834\n",
      "Epoch 6/50\n",
      "58538/58538 [==============================] - 332s 6ms/step - loss: 1.3175 - val_loss: 1.5839\n",
      "Epoch 7/50\n",
      "58538/58538 [==============================] - 333s 6ms/step - loss: 1.2971 - val_loss: 1.5801\n",
      "Epoch 8/50\n",
      "58538/58538 [==============================] - 334s 6ms/step - loss: 1.2802 - val_loss: 1.5762\n",
      "Epoch 9/50\n",
      "58538/58538 [==============================] - 332s 6ms/step - loss: 1.2653 - val_loss: 1.5837\n",
      "Epoch 10/50\n",
      "58538/58538 [==============================] - 332s 6ms/step - loss: 1.2519 - val_loss: 1.5884\n",
      "Epoch 00010: early stopping\n",
      "18293/18293 [==============================] - 31s 2ms/step\n",
      "Train on 58538 samples, validate on 14635 samples\n",
      "Epoch 1/50\n",
      "58538/58538 [==============================] - 1456s 25ms/step - loss: 1.8226 - val_loss: 1.6674\n",
      "Epoch 2/50\n",
      "58538/58538 [==============================] - 1455s 25ms/step - loss: 1.4006 - val_loss: 1.5792\n",
      "Epoch 3/50\n",
      "58538/58538 [==============================] - 1429s 24ms/step - loss: 1.3225 - val_loss: 1.5489\n",
      "Epoch 4/50\n",
      "58538/58538 [==============================] - 1430s 24ms/step - loss: 1.2738 - val_loss: 1.5475\n",
      "Epoch 5/50\n",
      "58538/58538 [==============================] - 1444s 25ms/step - loss: 1.2333 - val_loss: 1.5461\n",
      "Epoch 6/50\n",
      "58538/58538 [==============================] - 1445s 25ms/step - loss: 1.1950 - val_loss: 1.5530\n",
      "Epoch 7/50\n",
      "58538/58538 [==============================] - 1444s 25ms/step - loss: 1.1574 - val_loss: 1.5803\n",
      "Epoch 00007: early stopping\n",
      "18293/18293 [==============================] - 102s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "for h in hidden:\n",
    "    model = make_lstm_model(VOCAB_SIZE, DEFAULT_LAYERS, h, DEFAULT_DROPOUT)\n",
    "    model, history = run_model(model, X_train, y_train)\n",
    "    history['test_loss_predict'] = evaluate_loss(model, X_test)\n",
    "    history['test_loss_eval'] = model.evaluate(X_test, y_test, batch_size=30)\n",
    "    save_model(model, history, DEFAULT_LAYERS, h, DEFAULT_DROPOUT)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58538 samples, validate on 14635 samples\n",
      "Epoch 1/50\n",
      "58538/58538 [==============================] - 758s 13ms/step - loss: 2.0839 - val_loss: 1.7685\n",
      "Epoch 2/50\n",
      "58538/58538 [==============================] - 761s 13ms/step - loss: 1.5982 - val_loss: 1.6469\n",
      "Epoch 3/50\n",
      "58538/58538 [==============================] - 750s 13ms/step - loss: 1.4735 - val_loss: 1.6068\n",
      "Epoch 4/50\n",
      "58538/58538 [==============================] - 750s 13ms/step - loss: 1.4124 - val_loss: 1.5773\n",
      "Epoch 5/50\n",
      "58538/58538 [==============================] - 755s 13ms/step - loss: 1.3750 - val_loss: 1.5613\n",
      "Epoch 6/50\n",
      "58538/58538 [==============================] - 756s 13ms/step - loss: 1.3477 - val_loss: 1.5537\n",
      "Epoch 7/50\n",
      "58538/58538 [==============================] - 761s 13ms/step - loss: 1.3268 - val_loss: 1.5610\n",
      "Epoch 8/50\n",
      "58538/58538 [==============================] - 762s 13ms/step - loss: 1.3101 - val_loss: 1.5426\n",
      "Epoch 9/50\n",
      "58538/58538 [==============================] - 753s 13ms/step - loss: 1.2959 - val_loss: 1.5495\n",
      "Epoch 10/50\n",
      "58538/58538 [==============================] - 755s 13ms/step - loss: 1.2838 - val_loss: 1.5495\n",
      "Epoch 00010: early stopping\n",
      "18293/18293 [==============================] - 57s 3ms/step\n",
      "Train on 58538 samples, validate on 14635 samples\n",
      "Epoch 1/50\n",
      " 8550/58538 [===>..........................] - ETA: 10:21 - loss: 2.9554"
     ]
    }
   ],
   "source": [
    "for d in dropout:\n",
    "    model = make_lstm_model(VOCAB_SIZE, DEFAULT_LAYERS, DEFAULT_HIDDEN, d)\n",
    "    model, history = run_model(model, X_train, y_train)\n",
    "    history['test_loss_predict'] = evaluate_loss(model, X_test)\n",
    "    history['test_loss_eval'] = model.evaluate(X_test, y_test, batch_size=30)\n",
    "    save_model(model, history, DEFAULT_LAYERS, DEFAULT_HIDDEN, d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 58538 samples, validate on 14635 samples\n",
      "Epoch 1/50\n",
      "58538/58538 [==============================] - 767s 13ms/step - loss: 2.2032 - val_loss: 1.8457\n",
      "Epoch 2/50\n",
      "58538/58538 [==============================] - 765s 13ms/step - loss: 1.6906 - val_loss: 1.7047\n",
      "Epoch 3/50\n",
      "58538/58538 [==============================] - 766s 13ms/step - loss: 1.5517 - val_loss: 1.6432\n",
      "Epoch 4/50\n",
      "58538/58538 [==============================] - 762s 13ms/step - loss: 1.4843 - val_loss: 1.6049\n",
      "Epoch 5/50\n",
      "58538/58538 [==============================] - 763s 13ms/step - loss: 1.4434 - val_loss: 1.5945\n",
      "Epoch 6/50\n",
      "58538/58538 [==============================] - 762s 13ms/step - loss: 1.4149 - val_loss: 1.5806\n",
      "Epoch 7/50\n",
      " 2040/58538 [>.............................] - ETA: 11:38 - loss: 1.3885"
     ]
    }
   ],
   "source": [
    "# just run second dropout because it got stuck for some reason\n",
    "model = make_lstm_model(VOCAB_SIZE, DEFAULT_LAYERS, DEFAULT_HIDDEN, dropout[1])\n",
    "model, history = run_model(model, X_train, y_train)\n",
    "history['test_loss_predict'] = evaluate_loss(model, X_test)\n",
    "history['test_loss_eval'] = model.evaluate(X_test, y_test, batch_size=30)\n",
    "save_model(model, history, DEFAULT_LAYERS, DEFAULT_HIDDEN, dropout[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
